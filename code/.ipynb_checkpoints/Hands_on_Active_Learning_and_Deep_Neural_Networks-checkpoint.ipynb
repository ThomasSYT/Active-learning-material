{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics Seminar - Hands-on Session on Active Learning\n",
    "In the second part of this session, we are going to look at some non-deterministic classifiers, namely deep neural networks. Due to a random weight initialization at the begin of their training phase, they produce different results for different random seeds, even though the hyperparameters are the same. For active learning, we will again first compute the upper bound, then have a closer look at some things to watch out for when working with deep neural networks. This session requires keras and theano, two frameworks for training deep neural networks (there are plenty more).\n",
    "\n",
    "### General Set up\n",
    "Again, require some imports and have to set some configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import data_processing as dp\n",
    "\n",
    "config = {\n",
    "    'embedding':'embedding/glove.6B.50d.subset.oov.vec',\n",
    "    'train':'data/train.tsv',\n",
    "    'dev':'data/dev.tsv',\n",
    "    'test':'data/test.tsv',\n",
    "    'epochs':10, # Number of epochs to train for\n",
    "    'batch_size':5, # Our batch size for one backward pass\n",
    "    'random_seed':123456789, # Our random seed for the weight initialization\n",
    "    'optimizer':'adagrad', # The optimizer we want to use. Basically we can use everything from keras.\n",
    "    'model':'results/mlp-full' # The path to store our model in\n",
    "}\n",
    "\n",
    "# Add the random seed to our model path\n",
    "model_path = config['model'] + '-' + str(config['random_seed']) + '.model'\n",
    "# For keras and theano, it is ok to fix the numpy random seed.\n",
    "np.random.seed(config['random_seed'])\n",
    "weights_path = model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again load the same dataset for subjectivity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data.\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "#       Loading data and vectors\n",
    "###########################################\n",
    "\n",
    "embedding,embed_dim = dp.load_word2vec_embedding(config['embedding'])\n",
    "    \n",
    "X_train, y_train = dp.load_data(config['train'], textindex=1, labelindex=0)\n",
    "X_dev, y_dev = dp.load_data(config['dev'], textindex=1, labelindex=0)\n",
    "X_test, y_test = dp.load_data(config['test'], textindex=1, labelindex=0)\n",
    "\n",
    "# Get index-word/label dicts for lookup:\n",
    "vocab_dict = dp.get_index_dict(X_train + X_dev + X_test)\n",
    "label_dict = {'subjective':0, 'objective':1}\n",
    "\n",
    "# Replace words / labels in the data by the according index\n",
    "vocab_dict_flipped = dict((v,k) for k,v in vocab_dict.items())\n",
    "label_dict_flipped = {0:'subjective', 1:'objective'}\n",
    "\n",
    "# Get indexed data and labels\n",
    "X_train_index = [[vocab_dict_flipped[word] for word in chunk] for chunk in X_train]\n",
    "X_dev_index =  [[vocab_dict_flipped[word] for word in chunk] for chunk in X_dev]\n",
    "X_test_index =  [[vocab_dict_flipped[word] for word in chunk] for chunk in X_test]\n",
    "\n",
    "y_train_index = dp.get_binary_labels(label_dict, y_train)\n",
    "y_dev_index = dp.get_binary_labels(label_dict, y_dev)\n",
    "\n",
    "# Get embedding matrix:\n",
    "embed_matrix = dp.get_embedding_matrix(embedding,vocab_dict)\n",
    "\n",
    "# Use the simple count over all features in a single example:\n",
    "# Do average over word vectors:\n",
    "X_train_embedded = np.array([np.mean([embed_matrix[element] for element in example], axis=0) for example in X_train_index])\n",
    "X_dev_embedded = np.array([np.mean([embed_matrix[element] for element in example], axis=0) for example in X_dev_index])\n",
    "X_test_embedded = np.array([np.mean([embed_matrix[element] for element in example], axis=0) for example in X_test_index])\n",
    "\n",
    "print(\"Loaded data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks, we train several epochs (epoch = one full pass through our training set) with decreasing learning rates. Training several times on the same data with different learning rates helps the net to focus on different things in each epoch. After each epoch we evaluate our current model on the development set and store it if we have a best performing model. Following class implements this functionality which we can pass in keras to our fit() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for checking f1 measure during training\n",
    "class AccScore(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.best_acc = 0.0\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Get predictions\n",
    "        predict = np.asarray(self.model.predict(self.validation_data[0],batch_size=config['batch_size']))\n",
    "        # Flatten all outputs and remove padding\n",
    "        pred = []\n",
    "        true = []\n",
    "        for doc_pred,doc_true in zip(predict,self.validation_data[1]):\n",
    "            true.append(label_dict_flipped[doc_true.tolist().index(max(doc_true))])\n",
    "            pred.append(label_dict_flipped[doc_pred.tolist().index(max(doc_pred))])\n",
    "        self.accs=accuracy_score(pred, true)\n",
    "        if self.accs > self.best_acc:\n",
    "            self.best_acc=self.accs\n",
    "            model.save_weights(weights_path)\n",
    "        return\n",
    "\n",
    "accscore_met= AccScore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us implement a simple multi-layer perceptron. Since our data is rather low dimensional (one document is represented by an average of all 50-dimensional word vectors in it, we can keep the number of hidden units rather small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# A simple dense layer with 128 hidden units. The activation function is ReLU.\n",
    "model.add(Dense(128, activation='relu',input_shape=(embed_dim, ))) \n",
    "# Dropout acts as a regularizer to prevent overfitting.\n",
    "model.add(Dropout(0.4)) \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "# The final layer does the predcition. \n",
    "# Sigmoid is a common activation function for binary classifcation.\n",
    "model.add(Dense(2, activation='sigmoid')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our network on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.3858 - categorical_accuracy: 0.8434 - val_loss: 0.3022 - val_categorical_accuracy: 0.8710\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.3231 - categorical_accuracy: 0.8762 - val_loss: 0.2914 - val_categorical_accuracy: 0.8790\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.3159 - categorical_accuracy: 0.8782 - val_loss: 0.2869 - val_categorical_accuracy: 0.8840\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.3027 - categorical_accuracy: 0.8826 - val_loss: 0.2839 - val_categorical_accuracy: 0.8820\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.2945 - categorical_accuracy: 0.8832 - val_loss: 0.2812 - val_categorical_accuracy: 0.8890\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.2948 - categorical_accuracy: 0.8870 - val_loss: 0.2820 - val_categorical_accuracy: 0.8920\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.2929 - categorical_accuracy: 0.8844 - val_loss: 0.2783 - val_categorical_accuracy: 0.8790\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.2882 - categorical_accuracy: 0.8882 - val_loss: 0.2806 - val_categorical_accuracy: 0.8790\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.2941 - categorical_accuracy: 0.8874 - val_loss: 0.2752 - val_categorical_accuracy: 0.8830\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 1s - loss: 0.2878 - categorical_accuracy: 0.8880 - val_loss: 0.2722 - val_categorical_accuracy: 0.8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01e45d9f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first have to compile the model\n",
    "model.compile(config['optimizer'], 'binary_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "# Now we can train it:\n",
    "model.fit(X_train_embedded, y_train_index, epochs=config['epochs'], batch_size=config['batch_size'], validation_data=(X_dev_embedded, y_dev_index), verbose=1, callbacks=[accscore_met])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we load the best model on the dev set and compute the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8905\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(weights_path)\n",
    "result = model.predict(X_test_embedded)\n",
    "\n",
    "pred = []\n",
    "for i in range(len(result)):\n",
    "    pred.append(label_dict_flipped[result[i].tolist().index(max(result[i]))])\n",
    "\n",
    "print(\"Test accuracy: \",accuracy_score(pred, y_test))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So we have trained a simple neural network on our data. It even performs a bit better than the linear SVM using the same features. Now let's try out some different random seeds. What do you notice? Don't forget to write down the test scores together with the random seed for a comparison.\n",
    "\n",
    "### Deep Active Learning\n",
    "For active learning, we implement something similar to our support vector machine before. Keep in mind, how we have to set two different random seeds now, a python random seed for the random sampling and a numpy random seed for the deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of initial training documents:  2\n",
      "Loaded data.\n",
      "Training on:  50  instances.\n",
      "Training on:  100  instances.\n",
      "Training on:  150  instances.\n",
      "Training on:  200  instances.\n",
      "Training on:  250  instances.\n",
      "Training on:  300  instances.\n",
      "Training on:  350  instances.\n"
     ]
    }
   ],
   "source": [
    "from active_learning import Active_Learning\n",
    "\n",
    "active_learning_config = {\n",
    "    'embedding':'embedding/glove.6B.50d.subset.oov.vec',\n",
    "    'train_labeled':'data/train_labeled.tsv',\n",
    "    'train_unlabeled':'data/train_unlabeled.tsv',\n",
    "    'dev':'data/dev.tsv',\n",
    "    'test':'data/test.tsv',\n",
    "    'sampling':'random', # Sampling strategy, currently implemented ['random', 'confidence']\n",
    "    'c':2, # C for our SVM\n",
    "    'random_sampling_seed':42, # Random seed for the pseudo-randomnumber generator during random sampling\n",
    "    'maximum_iterations':500, # Maximum number of active learning iterations\n",
    "    'active_learning_history':'results/mlp-al-random.result', # File to store the results in\n",
    "    'epochs':10, # Number of epochs to train for\n",
    "    'batch_size':5, # Our batch size for one backward pass\n",
    "    'neural_network_random_seed':123456789, # Our random seed for the weight initialization\n",
    "    'optimizer':'adagrad', # The optimizer we want to use. Basically we can use everything from keras.\n",
    "    'model':'results/mlp-full' # The path to store our model in\n",
    "}\n",
    "\n",
    "# Add the random seed to our model path\n",
    "model_path = active_learning_config['model'] + '-' + str(active_learning_config['neural_network_random_seed']) + '.model'\n",
    "# For keras and theano, it is ok to fix the numpy random seed.\n",
    "np.random.seed(active_learning_config['neural_network_random_seed'])\n",
    "weights_path = model_path\n",
    "\n",
    "###########################################\n",
    "#       Loading data and vectors\n",
    "###########################################\n",
    "\n",
    "embedding,embed_dim = dp.load_word2vec_embedding(active_learning_config['embedding'])\n",
    "\n",
    "X_train, y_train = dp.load_data(active_learning_config['train_labeled'], textindex=1, labelindex=0)\n",
    "X_dev, y_dev = dp.load_data(active_learning_config['dev'], textindex=1, labelindex=0)\n",
    "X_test, y_test = dp.load_data(active_learning_config['test'], textindex=1, labelindex=0)\n",
    "\n",
    "# Active learning data\n",
    "X_active, y_active = dp.load_data(active_learning_config['train_unlabeled'], textindex=1, labelindex=0)\n",
    "\n",
    "# Get index-word/label dicts for lookup:\n",
    "# NOTE: Creating a dictionary out of all data has the implicit assumption \n",
    "#       that all the words we encounter during sampling and testing we have already seen during training.\n",
    "vocab_dict = dp.get_index_dict(X_train + X_test + X_active) \n",
    "label_dict = {'subjective':0, 'objective':1}\n",
    "\n",
    "# Replace words / labels in the data by the according index\n",
    "vocab_dict_flipped = dict((v,k) for k,v in vocab_dict.items())\n",
    "label_dict_flipped = {0:'subjective', 1:'objective'}\n",
    "\n",
    "# Get indexed data and labels\n",
    "X_train_index = [[vocab_dict_flipped[word] for word in chunk] for chunk in X_train]\n",
    "X_dev_index = [[vocab_dict_flipped[word] for word in chunk] for chunk in X_dev]\n",
    "X_test_index =  [[vocab_dict_flipped[word] for word in chunk] for chunk in X_test]\n",
    "\n",
    "# Active learning data\n",
    "X_active_index =  [[vocab_dict_flipped[word] for word in chunk] for chunk in X_active]\n",
    "\n",
    "print (\"Number of initial training documents: \",len(X_train))\n",
    "\n",
    "# Get embedding matrix:\n",
    "embed_matrix = dp.get_embedding_matrix(embedding,vocab_dict)\n",
    "\n",
    "# Use the simple count over all features in a single example:\n",
    "# Do average over word vectors:\n",
    "X_train_embedded = np.array([np.mean([embed_matrix[element] for element in example], axis=0) for example in X_train_index])\n",
    "X_dev_embedded = np.array([np.mean([embed_matrix[element] for element in example], axis=0) for example in X_dev_index])\n",
    "X_test_embedded = np.array([np.mean([embed_matrix[element] for element in example], axis=0) for example in X_test_index])\n",
    "\n",
    "# Active learning\n",
    "X_active_embedded = np.array([np.mean([embed_matrix[element] for element in example], axis=0) for example in X_active_index])\n",
    "\n",
    "y_train_index = dp.get_binary_labels(label_dict, y_train)\n",
    "y_dev_index = dp.get_binary_labels(label_dict, y_dev)\n",
    "y_active_index = dp.get_binary_labels(label_dict, y_active)\n",
    "\n",
    "# Define our pools for active learning\n",
    "pool_data = X_active_embedded[:]\n",
    "pool_labels = y_active_index[:]\n",
    "\n",
    "print(\"Loaded data.\")\n",
    "\n",
    "# Class for checking f1 measure during training\n",
    "class AccScore(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.best_acc = 0.0\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Get predictions\n",
    "        predict = np.asarray(self.model.predict(self.validation_data[0],batch_size=active_learning_config['batch_size']))\n",
    "        # Flatten all outputs and remove padding\n",
    "        pred = []\n",
    "        true = []\n",
    "        for doc_pred,doc_true in zip(predict,self.validation_data[1]):\n",
    "            true.append(label_dict_flipped[doc_true.tolist().index(max(doc_true))])\n",
    "            pred.append(label_dict_flipped[doc_pred.tolist().index(max(doc_pred))])\n",
    "        self.accs=accuracy_score(pred, true)\n",
    "        if self.accs > self.best_acc:\n",
    "            self.best_acc=self.accs\n",
    "            model.save_weights(weights_path)\n",
    "        return\n",
    "\n",
    "accscore_met= AccScore()\n",
    "\n",
    "###########################################\n",
    "#       Implement model\n",
    "###########################################\n",
    "\n",
    "model = Sequential()\n",
    "# A simple dense layer with 128 hidden units. The activation function is ReLU.\n",
    "model.add(Dense(128, activation='relu',input_shape=(embed_dim, ))) \n",
    "# Dropout acts as a regularizer to prevent overfitting.\n",
    "model.add(Dropout(0.4)) \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "# The final layer does the predcition. \n",
    "# Sigmoid is a common activation function for binary classifcation.\n",
    "model.add(Dense(2, activation='sigmoid')) \n",
    "\n",
    "###########################################\n",
    "#       Compile the model\n",
    "###########################################\n",
    "# We first have to compile the model\n",
    "model.compile(active_learning_config['optimizer'], 'binary_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "###########################################\n",
    "#       Start active learning\n",
    "###########################################\n",
    "# Active learning results for visualization\n",
    "step, acc = [],[]\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "outlog = open(active_learning_config['active_learning_history'],'w')\n",
    "outlog.write('Iteration\\tAccuracy\\n')\n",
    "\n",
    "while len(pool_data) > 1 and iteration < active_learning_config['maximum_iterations']:\n",
    "    if len(X_train_embedded) % 50 == 0:\n",
    "        print(\"Training on: \", len(X_train_embedded), \" instances.\")\n",
    "\n",
    "    model.fit(X_train_embedded, y_train_index, epochs=active_learning_config['epochs'], batch_size=active_learning_config['batch_size'], validation_data=(X_dev_embedded, y_dev_index), verbose=0, callbacks=[accscore_met])\n",
    "\n",
    "    # Load best weights and compute test performance\n",
    "    model.load_weights(weights_path)\n",
    "    result = model.predict(X_test_embedded)\n",
    "    pred = []\n",
    "    for i in range(len(result)):\n",
    "        pred.append(label_dict_flipped[result[i].tolist().index(max(result[i]))])\n",
    "    test_acc = accuracy_score(y_test,pred)\n",
    "    outlog.write('{}\\t{}\\n'.format(iteration,test_acc))\n",
    "    step.append(iteration); acc.append(test_acc)\n",
    "\n",
    "    # Add data from the pool to the training set based on our active learning:\n",
    "    al = Active_Learning(pool_data, model, active_learning_config['random_sampling_seed'])\n",
    "    if active_learning_config['sampling'] == 'random':\n",
    "        add_sample_data = al.get_random()\n",
    "    else:\n",
    "        add_sample_data = al.get_most_uncertain(active_learning_config['sampling'])\n",
    "\n",
    "    # Get the data index from pool\n",
    "    sample_index = dp.get_array_index(pool_data, add_sample_data)\n",
    "        \n",
    "    # Get the according label\n",
    "    add_sample_label = pool_labels[sample_index]\n",
    "\n",
    "    # Add it to the training pool\n",
    "    X_train_embedded = np.vstack((X_train_embedded, add_sample_data))\n",
    "    y_train_index = np.vstack((y_train_index, add_sample_label))\n",
    "\n",
    "    # Remove labeled data from pool\n",
    "    np.delete(pool_labels, sample_index, axis=0) \n",
    "    np.delete(pool_data, sample_index, axis=0)\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "outlog.close()\n",
    "\n",
    "print(\"Done with active learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, how the training time really increases a lot? Again, let's plot the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import visualize as vz\n",
    "\n",
    "vz.plot(step, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it! The code is also provided in proper classes in the code folder. Feel free to experiment with it or modify it for other purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
